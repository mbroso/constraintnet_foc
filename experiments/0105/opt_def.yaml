options:
  opts_algorithm:
    batch_size:
      args:
      - --batch_size
      kwargs:
        default: 256
        help: Batchsize of training samples
        type: &id002 !!python/name:builtins.int ''
    discount:
      args:
      - --discount
      kwargs:
        default: 0.99
        help: Discount factor
        type: &id001 !!python/name:builtins.float ''
    expl_noise:
      args:
      - --expl_noise
      kwargs:
        default: 0.1
        help: Standard deviation of exploration noise = expl_noise * max_action
        type: *id001
    neurons:
      args:
      - --neurons
      kwargs:
        default: 256
        help: Number of neurons per layer in actor and critic
        type: *id002
    replaybuffer_size:
      args:
      - --replaybuffer_size
      kwargs:
        default: 1000000
        help: Size of queue implementing the experience replay buffer
        type: *id002
    tau:
      args:
      - --tau
      kwargs:
        default: 0.005
        help: Factor for soft update of networks
        type: *id001
  opts_algorithm_TD3:
    noise_clip:
      args:
      - --noise_clip
      kwargs:
        default: 0.5
        help: Clip training noise
        type: *id001
    policy_freq:
      args:
      - --policy_freq
      kwargs:
        default: 2
        help: Update actor every N critic updates.
        type: *id002
    policy_noise:
      args:
      - --policy_noise
      kwargs:
        default: 0.2
        help: Standard deviation of noise which is added to actions during training
        type: *id001
  opts_constraints:
    actor_type:
      args:
      - --actor_type
      kwargs:
        choices:
        - unconstrained
        - ContraintNet
        default: unconstrained
        help: Which actor to use. Can be either "unconstrained", "constrained". Has
          effect if TD3 policy is used.
    constraints_lower:
      args:
      - --constraints_lower
      kwargs:
        default: ''
        help: Lower constraints. Can be combination of iso and no_backwards. e.g.
          iso+no_backwards
    constraints_upper:
      args:
      - --constraints_upper
      kwargs:
        default: ''
        help: Upper constraints. e.g. no_crash
    cstr_no_backwards_jerk:
      args:
      - --cstr_no_backwards_jerk
      kwargs:
        help: Absolute value of maximum jerk allowed by vehicle in m/s^3 to prevent
          vehicle from going backwards
        type: *id001
    cstr_no_crash_a_min:
      args:
      - --cstr_no_crash_a_min
      kwargs:
        help: Lowest allowed acceleration by car (negative value) for no crash constraint
          in m/s^2
        type: *id001
    cstr_no_crash_jerk:
      args:
      - --cstr_no_crash_jerk
      kwargs:
        help: Absolute value of maximum jerk for no crash constraint in m/s^3
        type: *id001
    cstr_no_crash_tg_min:
      args:
      - --cstr_no_crash_tg_min
      kwargs:
        help: Lowest allowed timegap for no crash constraint in s
        type: *id001
    polys_convex_polys_v_dim:
      args:
      - --polys_convex_polys_v_dim
      kwargs:
        help: Specify the number of dimensions for each convex polytope. Concat for
          all convex polytopes within one output part and for all output parts. Should
          be a list with the same length as polys_convex_polys_v_n.
        nargs: +
    polys_convex_polys_v_n:
      args:
      - --polys_convex_polys_v_n
      kwargs:
        help: Specify the number of vertices for each convex polytope. Concat for
          all convex polytopes within one output part and for all output parts.
        nargs: +
    polys_output_parts:
      args:
      - --polys_output_parts
      kwargs:
        help: Specify the number of convex polytopes for each output part. Should
          be a list with same length as polys_convex_polys_v_n.
        nargs: +
  opts_environment:
    clip_a_dem:
      args:
      - --clip_a_dem
      kwargs:
        action: store_true
        help: Clipping of a_dem according to constraints. (for unconstrained model)
    desired_headway:
      args:
      - --desired_headway
      kwargs:
        help: Desired headway (time gap) to target vehicle in seconds
        type: *id001
    env:
      args:
      - --env
      kwargs:
        help: Name of Gym environment.
    env_dt:
      args:
      - --env_dt
      kwargs:
        default: 0.1
        help: Time interval of the environment. (Environment time is increased by
          this value every env.step())
        type: *id001
    env_stop_time:
      args:
      - --env_stop_time
      kwargs:
        default: 3600
        help: Maximum duration of one episode in seconds
        type: *id001
    observations:
      args:
      - --observations
      kwargs:
        help: Which obervations to use. Order matters
        nargs: +
    reward_control_jerk_weight:
      args:
      - --reward_control_jerk_weight
      kwargs:
        help: Weight of cost for change in commanded acceleration
        type: *id001
    reward_control_weight:
      args:
      - --reward_control_weight
      kwargs:
        help: Weight of cost for commanded acceleration
        type: *id001
    reward_function:
      args:
      - --reward_function
      kwargs:
        choices:
        - CACC Paper
        - CACC Paper continuous
        - custom
        help: Which reward function shall be used
    sim_dt:
      args:
      - --sim_dt
      kwargs:
        default: 0.1
        help: Time interval of the underlaying pyhsics simulation. (env_dt has to
          be a multiple of sim_dt)
        type: *id001
    stop_n_go_distance:
      args:
      - --stop_n_go_distance
      kwargs:
        default: 3.2
        help: Desired distance to target vehicle when ego vehicle comes to a stop
          in meters
        type: *id001
    stop_n_go_velocity:
      args:
      - --stop_n_go_velocity
      kwargs:
        default: 3.3333
        help: At speeds below this threshold, desired distance to target vehicle is
          increased for stop & go scenario. Velocity in m/s.
        type: *id001
    traffic_scenario:
      args:
      - --traffic_scenario
      kwargs:
        choices:
        - SimpleSine
        - RampsAndHold
        - RampsSineAndHold
        help: Which traffic scenario to use
    vehicle_a_max:
      args:
      - --vehicle_a_max
      kwargs:
        help: Maximum acceleration of ego-vehicle in m/s^2.
        type: *id001
    vehicle_a_min:
      args:
      - --vehicle_a_min
      kwargs:
        help: Minimal acceleration of ego-vehicle in m/s^2.
        type: *id001
    vehicle_model:
      args:
      - --vehicle_model
      kwargs:
        choices:
        - Simples
        - RealData
        default: Simple
        help: Which vehicle model to use. Either use a simple model or a more realistic
          one aquired from real data
  opts_general:
    comment:
      args:
      - --comment
      - --cmt
      kwargs:
        default: ''
        help: User specified comment.
    experiment_id:
      args:
      - --experiment_id
      - --id
      kwargs:
        help: Experiment id to uniquely identify each experiment
        type: *id002
    num_threads:
      args:
      - --num_threads
      kwargs:
        default: 2
        help: Number of threads to use for pytorch training
        type: *id002
    seed:
      args:
      - --seed
      kwargs:
        default: 0
        help: Random seed for reproducibility. Used for seeding pytorch and Gym environment
        type: *id002
  opts_render:
    plot_grid:
      args:
      - --plot_grid
      kwargs:
        action: store_true
        help: Whether to use a grid in the plots.
    plot_titles:
      args:
      - --plot_titles
      kwargs:
        help: Titles of the suplots specified by plot_variables. Latex code can be
          used, but must be encapsulated in quotes.
        nargs: +
    plot_variables:
      args:
      - --plot_variables
      kwargs:
        help: Which variables to plot. Possible values are all members of the state
          dict in AccFocEnv.py. Multiple variables can be plotted in one subplot by
          joining them with a + (e.g. a_ego+a_tar).
        nargs: +
    render:
      args:
      - --render
      kwargs:
        action: store_true
        help: Whether rendering of completed episodes is enabled or not
    render_min_interval:
      args:
      - --render_min_interval
      kwargs:
        default: 1000
        help: An episode is rendered if atleast render_min_interval training steps
          have passed since last render. Limits the number of total renders
        type: *id002
  opts_results:
    save_model:
      args:
      - --save_model
      kwargs:
        action: store_true
        help: Whether to store the trained model or not
  opts_traffic_scenario:
    RampsAndHold_random:
      args:
      - --RampsAndHold_random
      kwargs:
        help: Parameters for randomization of RampsAndHold Environment. [Random yes/no,
          T_min, T_max, Probability of randomizing period afer each period]
        nargs: +
        type: *id001
    cut_in_cut_out_random:
      args:
      - --cut_in_cut_out
      kwargs:
        help: Parameters for random cut in and cut out. [Probability, Hw_min, Hw_max,
          v_change]
        nargs: +
        type: *id001
    traffic_scenario_start_ideal:
      args:
      - --traffic_scenario_start_ideal
      kwargs:
        action: store_true
        help: Start in ideal distance and with same velocity as target vehicle
    traffic_scenario_vehicle_time_constant:
      args:
      - --traffic_scenario_vehicle_time_constant
      kwargs:
        default: 0.5
        help: Time constant of vehicle model in synthetic traffic scenario
        type: *id001
  opts_training:
    eval_freq:
      args:
      - --eval_freq
      kwargs:
        default: 5000
        help: Evaluate every N training timesteps
        type: *id002
    evaluation_episodes:
      args:
      - --evaluation_episodes
      kwargs:
        default: 5
        help: Number of episodes of each evalution
        type: *id002
    load_model:
      args:
      - --load_model
      kwargs:
        help: Option to load a pretrained model
    max_timesteps:
      args:
      - --max_timesteps
      kwargs:
        default: 1000000
        help: Number of total training timesteps
        type: *id002
    normalize_data:
      args:
      - --normalize_data
      kwargs:
        default: false
        help: Whether to normalize (mean=0, std=1) the observations or not
        type: !!python/name:builtins.bool ''
    policy:
      args:
      - --policy
      kwargs:
        default: TD3
        help: 'Policy, e.g: TD3, DDPG, OurDDPG'
    start_timesteps:
      args:
      - --start_timesteps
      kwargs:
        default: 10000
        help: Number of inital random timestamps after which training is started
        type: *id002
